# -*- coding: utf-8 -*-
"""final_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PRljs8MbiwcCMeYKzgKgy72rPmSrW23B
"""

# Check for GPU availability
import tensorflow as tf
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
print("TensorFlow is using GPU:", tf.test.is_built_with_cuda())

import tensorflow as tf

# Set GPU as the default device
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        tf.config.set_visible_devices(gpus[0], 'GPU')
        print("Using GPU:", gpus[0])
    except RuntimeError as e:
        print(e)

# Clone the repository
!git clone https://github.com/Bhavnoor-Coders-1010/food101

# Import libraries
import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.applications import VGG16
import matplotlib.pyplot as plt
from google.colab import files

# Step 1: Define dataset path
dataset_path = "/content/food101/DataFOOD101"
classes = os.listdir(dataset_path)  # Assuming each class has its own folder

# Step 2: Load and preprocess images
img_size = (150, 150)
X, Y = [], []

for class_index, class_name in enumerate(classes):
    class_path = os.path.join(dataset_path, class_name)
    # Check if the path is a directory before proceeding
    if os.path.isdir(class_path):
        for img_name in os.listdir(class_path):
            img_path = os.path.join(class_path, img_name)
            img = cv2.imread(img_path)
            if img is not None:
                img = cv2.resize(img, img_size)
                X.append(img)
                Y.append(class_index)

X = np.array(X, dtype='float32') / 255.0  # Normalize image pixel values
Y = np.array(Y)

# Step 3: One-hot encode labels
Y = to_categorical(Y, num_classes=len(classes))

# Step 4: Split into train and test sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Step 5: Load pre-trained VGG16 model
conv_base = VGG16(
    weights='imagenet',
    include_top=False,
    input_shape=(150, 150, 3)
)

# Fine-tune specific layers
conv_base.trainable = True
set_trainable = False
for layer in conv_base.layers:
    if layer.name == 'block5_conv1':  # Unfreeze from block5_conv1 onwards
        set_trainable = True
    layer.trainable = set_trainable

conv_base.summary()

# Step 6: Define the sequential model
model = Sequential([
    conv_base,
    Flatten(),
    Dense(256, activation='relu'),
    Dense(len(classes), activation='softmax')  # Multi-class classification
])

# Step 7: Compile the model
model.compile(
    optimizer='rmsprop',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Step 8: Train the model
history = model.fit(
    X_train, Y_train,
    epochs=10,
    validation_data=(X_test, Y_test),
    batch_size=32
)

model.save('trained_model.h5')
print("Model saved as 'trained_model.h5'")
files.download('trained_model.h5')

# Step 9: Plot training history
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], color='red', label='Train Accuracy')
plt.plot(history.history['val_accuracy'], color='blue', label='Validation Accuracy')
plt.title('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], color='red', label='Train Loss')
plt.plot(history.history['val_loss'], color='blue', label='Validation Loss')
plt.title('Loss')
plt.legend()

plt.show()